{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cff24d05",
   "metadata": {},
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fd0d0d0",
   "metadata": {},
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee3e0d",
   "metadata": {},
   "source": [
    "# 1. Import the datasets and libraries, check shape, and datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b850d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maher\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maher\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "       vidid  adview    views likes dislikes comment   published duration  \\\n",
      "0  VID_18655      40  1031602  8523      363    1095  2016-09-14  PT7M37S   \n",
      "1  VID_14135       2     1707    56        2       6  2016-10-01  PT9M30S   \n",
      "2   VID_2187       1     2023    25        0       2  2016-07-02  PT2M16S   \n",
      "3  VID_23096       6   620860   777      161     153  2016-07-27  PT4M22S   \n",
      "4  VID_10175       1      666     1        0       0  2016-06-29    PT31S   \n",
      "\n",
      "  category  \n",
      "0        F  \n",
      "1        D  \n",
      "2        C  \n",
      "3        H  \n",
      "4        D  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "# Load the dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510c4399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14999, 9)\n",
      "vidid        object\n",
      "adview        int64\n",
      "views        object\n",
      "likes        object\n",
      "dislikes     object\n",
      "comment      object\n",
      "published    object\n",
      "duration     object\n",
      "category     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check shape and datatype\n",
    "print(df.shape)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe91229",
   "metadata": {},
   "source": [
    "# 2. Visualise the dataset using plotting using heatmaps and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36226c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "print(df.corr())\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Visualize data distributions\n",
    "df.hist(bins=20, figsize=(15, 10))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968733e",
   "metadata": {},
   "source": [
    "# 3. Clean the dataset by removing missing values and other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2cc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "df.dropna(inplace=True)\n",
    "# Convert 'views', 'likes', 'dislikes', 'comment' to numeric\n",
    "df['views'] = pd.to_numeric(df['views'], errors='coerce')\n",
    "df['likes'] = pd.to_numeric(df['likes'], errors='coerce')\n",
    "df['dislikes'] = pd.to_numeric(df['dislikes'], errors='coerce')\n",
    "df['comment'] = pd.to_numeric(df['comment'], errors='coerce')\n",
    "# Convert 'published' to datetime\n",
    "df['published'] = pd.to_datetime(df['published'])\n",
    "\n",
    "# Transform 'category' using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['category'], drop_first=True)\n",
    "# Extract 'duration' and convert to seconds\n",
    "# Convert 'duration' to seconds\n",
    "def duration_to_seconds(duration_str):\n",
    "    duration_match = re.match(r'PT(\\d+H)?(\\d+M)?(\\d+S)?', duration_str)\n",
    "\n",
    "    if duration_match:\n",
    "        hours = int(duration_match.group(1)[:-1]) if duration_match.group(1) else 0\n",
    "        minutes = int(duration_match.group(2)[:-1]) if duration_match.group(2) else 0\n",
    "        seconds = int(duration_match.group(3)[:-1]) if duration_match.group(3) else 0\n",
    "        return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "    return 0\n",
    "\n",
    "# Apply the function to convert 'duration' to seconds\n",
    "df['duration_sec'] = df['duration'].apply(duration_to_seconds)\n",
    "\n",
    "# Drop the original 'duration' column\n",
    "df.drop(['duration'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1d43c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       vidid  adview      views   likes  dislikes  comment  published  \\\n",
      "0  VID_18655      40  1031602.0  8523.0     363.0   1095.0 2016-09-14   \n",
      "1  VID_14135       2     1707.0    56.0       2.0      6.0 2016-10-01   \n",
      "2   VID_2187       1     2023.0    25.0       0.0      2.0 2016-07-02   \n",
      "3  VID_23096       6   620860.0   777.0     161.0    153.0 2016-07-27   \n",
      "4  VID_10175       1      666.0     1.0       0.0      0.0 2016-06-29   \n",
      "\n",
      "   category_B  category_C  category_D  category_E  category_F  category_G  \\\n",
      "0           0           0           0           0           1           0   \n",
      "1           0           0           1           0           0           0   \n",
      "2           0           1           0           0           0           0   \n",
      "3           0           0           0           0           0           0   \n",
      "4           0           0           1           0           0           0   \n",
      "\n",
      "   category_H  duration_sec  \n",
      "0           0           457  \n",
      "1           0           570  \n",
      "2           0           136  \n",
      "3           1           262  \n",
      "4           0            31  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc605d",
   "metadata": {},
   "source": [
    "# 4. Transform attributes into numerical values and other necessary transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965623eb",
   "metadata": {},
   "source": [
    "# 5. Normalize your data and split the data into training, validation, and test set in the appropriate ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c77b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maher\\AppData\\Local\\Temp\\ipykernel_18128\\221579314.py:2: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  df.fillna(df.mean(), inplace=True)\n",
      "C:\\Users\\maher\\AppData\\Local\\Temp\\ipykernel_18128\\221579314.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df.fillna(df.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Replace NaN with the mean of the corresponding column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Replace infinity values with a large finite value\n",
    "df.replace([np.inf, -np.inf], np.finfo('float64').max, inplace=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(['vidid', 'adview', 'published'], axis=1)\n",
    "y = df['adview']\n",
    "\n",
    "# Normalize data excluding 'published'\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Combine normalized features and 'published' column\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "X_combined = pd.concat([X_scaled_df, df[['published']]], axis=1)\n",
    "\n",
    "# Convert 'published' to numeric features\n",
    "X_combined['published_year'] = X_combined['published'].dt.year\n",
    "X_combined['published_month'] = X_combined['published'].dt.month\n",
    "X_combined['published_day'] = X_combined['published'].dt.day\n",
    "X_combined.drop(['published'], axis=1, inplace=True)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_combined, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaabd3cf",
   "metadata": {},
   "source": [
    "# 6. Use linear regression, Support Vector Regressor for training and get errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b08511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Mean Squared Error: 13314200621.970387\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_preds = lr_model.predict(X_val)\n",
    "lr_error = mean_squared_error(y_val, lr_preds)\n",
    "print(\"Linear Regression Mean Squared Error:\", lr_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d8ef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Mean Squared Error: 13355366737.251987\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Regressor\n",
    "svr_model = SVR()\n",
    "svr_model.fit(X_train, y_train)\n",
    "svr_preds = svr_model.predict(X_val)\n",
    "svr_error = mean_squared_error(y_val, svr_preds)\n",
    "print(\"SVR Mean Squared Error:\", svr_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901ab79",
   "metadata": {},
   "source": [
    "# 7. Use Decision Tree Regressor and Random Forest Regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba83ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean Squared Error: 14670885157.435556\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_preds = dt_model.predict(X_val)\n",
    "dt_error = mean_squared_error(y_val, dt_preds)\n",
    "print(\"Decision Tree Mean Squared Error:\", dt_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd898c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Mean Squared Error: 12871287119.847685\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_val)\n",
    "rf_error = mean_squared_error(y_val, rf_preds)\n",
    "print(\"Random Forest Mean Squared Error:\", rf_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a1a87",
   "metadata": {},
   "source": [
    "# 8. Build an artificial neural network and train it with different layers and hyperparameters. Experiment a little. Use Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "152996de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maher\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcf33e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06ca33d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\maher\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "329/329 [==============================] - 5s 8ms/step - loss: 818293248.0000 - val_loss: 13344019456.0000\n",
      "Epoch 2/10\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 818262080.0000 - val_loss: 13344801792.0000\n",
      "Epoch 3/10\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 818210816.0000 - val_loss: 13345193984.0000\n",
      "Epoch 4/10\n",
      "329/329 [==============================] - 2s 5ms/step - loss: 818185280.0000 - val_loss: 13344849920.0000\n",
      "Epoch 5/10\n",
      "329/329 [==============================] - 2s 5ms/step - loss: 818228160.0000 - val_loss: 13344452608.0000\n",
      "Epoch 6/10\n",
      "329/329 [==============================] - 2s 5ms/step - loss: 818230784.0000 - val_loss: 13344352256.0000\n",
      "Epoch 7/10\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 818172160.0000 - val_loss: 13344847872.0000\n",
      "Epoch 8/10\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 818206656.0000 - val_loss: 13344277504.0000\n",
      "Epoch 9/10\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 818142848.0000 - val_loss: 13345565696.0000\n",
      "Epoch 10/10\n",
      "329/329 [==============================] - 2s 6ms/step - loss: 818182528.0000 - val_loss: 13345399808.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x251ce59cdc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78704d",
   "metadata": {},
   "source": [
    "# 9. Pick the best model based on error as well as generalization.\n",
    "Compare the errors obtained from different models and choose the one with the lowest validation error and good generalization.\n",
    "\n",
    "10. Save your model and predict on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45b75125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Mean Squared Error: 1141370214.6938925\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "best_model = rf_model  \n",
    "# Predict on the test set\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_error = mean_squared_error(y_test, test_preds)\n",
    "print(\"Test Set Mean Squared Error:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d0145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
